---
title: "Rapport TP2 - Partie C"
author: "Hubert Hirtz, Camille Schnell"
date: "10 décembre 2018"
output: pdf_document
---

#Objectif
Il s'agit ici, à partir du dataset *spam* (librairie **ElemStatLearn**), de comparer les performances de différentes machines, basées sur les modèles étudiés en cours.

#Mise en œuvre

Les données de spam sont divisées en cinq parties grâce à `vfold_cv`.

Pour chacune de ces parties `x.train`, on prend le reste des données `x.test`.

Les données d'entrainnement sont utilisées pour construire les modèles de prédiction :

- `tree(spam~., data = x.train)`
- `bagging(spam~., data = x.train)`
- `randomForest(spam~., data = x.train)`
- ...

Ces modèles sont ensuite utilisés pour prédire les spams des données de test :

`y_pred <- predict(model, newdata = x.test, type = "class")`

`y_pred` contient alors les prédictions du modèle `model` selon les 57 premières colonnes de `x.test`.
On calcule avec la moyenne des bonnes prédictions avec `mean(y_pred == x.test$spam)`.

```{r}
#chargement des librairies
library(randomForest)
library(MASS)
library(tidyr)
library(rsample)
library(rpart)
library(tree)
library(e1071)
library(ipred)
library(ElemStatLearn)
library(purrr)
library(tibble)
library(ggplot2)

#réinitialisation des données dans rstudio
rm(list=ls());
graphics.off();

#récupération des données de spam
data(spam)

#analyse des performances
folds <- vfold_cv(spam, v=5)
performance <- map_dfr(folds$splits,
       function(x){
         x.train <- as_tibble(x, data = "analysis")
         x.test <- as_tibble(x, data = "assessment")
         
         #CART
         y_pred <- predict(tree(spam~., data = x.train), newdata = x.test,
                           type = "class")
         Tree <- mean(y_pred == x.test$spam)
         
         #Bagging
         y_pred <- predict(bagging(spam~., data = x.train), newdata = x.test,
                           type = "class")
         Bagging <- mean(y_pred == x.test$spam)
         
         #Random forest
         y_pred <- predict(randomForest(spam~., data = x.train), newdata = x.test,
                           type = "class")
         RandomForest <- mean(y_pred == x.test$spam)
         
         #LDA
         y_pred <- predict(lda(spam~., data = x.train), newdata = x.test,
                           type = "class")$class
         LDA <- mean(y_pred == x.test$spam)
         
         #QDA
         # Error in qda.default(x, grouping, ...) : rank deficiency in group spam
         #y_pred <- predict(qda(spam~., data = x.train), newdata = x.test)
         #QDA <- mean(y_pred == x.test$spam)
         
         #Logistic regression
         y_pred <- predict(glm(spam~., family = binomial(), data = x.train),
                           newdata = x.test, type="response")
         LogReg <- mean(y_pred == x.test$spam)
         
         #Bayes
         y_pred <- predict(naiveBayes(spam~., data = x.train), newdata = x.test)
         Bayes <- mean(y_pred == x.test$spam)
         
         tibble(Bayes=Bayes, CART=Tree, LDA=LDA, LogReg=LogReg,
                Bagging=Bagging, RandomForest=RandomForest)
         
       }
      )
```

#Résultats

Les données de préférences, qui sont sous forme d'un tableau avec en colonne l'algorithme et en ligne la performance, sont réordonnées en deux colonnes : l'algorithme et le taux d'erreur.

Elles sont ensuite affichées sous forme de boxplots.

```{r}
#Affichage des résultats sous forme de boxplots
performance <- gather(performance, key="Algorithm", value="Error")
ggplot(data = performance,aes(x=Algorithm, y=Error,col=Algorithm)) + geom_boxplot()
```


#Conclusion
Nous remarquons, en comparant les résultats des 7 différentes machines, que l'algorithme *Random Forest* est le plus performant.